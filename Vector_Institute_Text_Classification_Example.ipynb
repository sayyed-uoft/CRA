{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayyed-uoft/CRA/blob/main/Vector_Institute_Text_Classification_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVZSsLGuF_lh"
   },
   "source": [
    "# Vector Institute - CRA AI Fundamentals and Ethical Considerations Workshop\n",
    "\n",
    "### Thank you for joining Day 2 of the Vector Institute, 'AI Fundamentals and Ethical Considerations' workshop series.\n",
    "\n",
    "If you have any questions or if you would like to learn more about this program, contact: learn@vectorinstitute.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izA3-6kffbdT"
   },
   "source": [
    "# Case Study 2: Text Classification\n",
    "\n",
    "\n",
    "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of narratives from consumer complaints. And we will calssify each narrative to one of the compalint classes.\n",
    "\n",
    "## Models: Consumer Complaint Classification\n",
    "Our goal is to create a model that takes the text of a compliant and produces the class code. \n",
    "\n",
    "Under the hood, the model is actually made up of two model.\n",
    "\n",
    "* DistilBERT processes the text and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
    "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the text. We will train both binary and multi-class calssifiers and will explain the methods to evaluate the results..\n",
    "\n",
    "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
    "\n",
    "## Problem\n",
    "Each week the Consumer Financial Protection Bureau sends thousands of consumer’s complaints about financial product and services to company for a response. Classify those consumer complaints into the product category it belongs to using the description of the complaint.\n",
    "\n",
    "## Dataset\n",
    "The dataset is a small subset of data extracted from Data.gov website. We extracted only a very small part due to memory limitation of Google Colab.  The data is already clean. It is made of two columns:\n",
    "\n",
    "1. **Product:** the complaint class\n",
    "1. **Consumer complaint narrative:** the text of the complaint\n",
    "\n",
    "## Installing the transformers library\n",
    "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model. Also, importing the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To9ENLU90WGl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvFvBLJV0Dkv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Importing the dataset\n",
    "We'll use pandas to read the dataset and load it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyoj29J24hPX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/sayyed-uoft/CRA/main/customer_complaints_samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMVE3waNhuNj"
   },
   "source": [
    "Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRc2L89hh1Tf"
   },
   "source": [
    "Let's look at the distribution of the products (labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGvcfcCP5xpZ"
   },
   "outputs": [],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-jk7LO0fZ4l"
   },
   "outputs": [],
   "source": [
    "df['Product'].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Loading the Pre-trained BERT model\n",
    "Let's now load a pre-trained BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1InADgf5xm2"
   },
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZDBMn3wiSX6"
   },
   "source": [
    "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
    "\n",
    "## Preparing the Dataset\n",
    "Before we can hand our narratives to BERT, we need to do some minimal processing to put them in the format it requires.\n",
    "\n",
    "### Tokenization\n",
    "Our first step is to tokenize the narratives -- break them up into word and subwords in the format BERT is comfortable with. The model accepts obly 512 tockens. So, we truncate longer messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "# Tokenize the narratives\n",
    "tokenized = df['Consumer complaint narrative'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqYGumGIkO56"
   },
   "outputs": [],
   "source": [
    "# view a few tockenized samples\n",
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHwjUwYgi-uL"
   },
   "source": [
    "\n",
    "### Padding\n",
    "After tokenization, `tokenized` is a list of narratives -- each narrative is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "Our dataset is now in the `padded` variable, we can view its dimensions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdi7uXo95xeq"
   },
   "outputs": [],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K_iGRNa_Ozc"
   },
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK-CQB9-kN99"
   },
   "source": [
    "## Narrative Embeddings\n",
    "Now that we have our model and inputs ready, let's run our model!\n",
    "\n",
    "The `model()` function runs our narratives through BERT. The results of the processing will be returned into `last_hidden_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "# Note: This will take a while\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoCep_WVuB3v"
   },
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "\n",
    "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9t60At16PVs"
   },
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJhSb5RLNOF4"
   },
   "source": [
    "## Classification\n",
    "\n",
    "The last step is to use the narrative representations as the input of a simple linear classification model. For the output we will use the index of the associated categories.\n",
    "\n",
    "We will train two models:\n",
    "1. Binary classification (if the complaint’s type is ‘Credit reporting, credit repair services, or other personal consumer reports’)\n",
    "1. Multi-class classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61I2QBXPOMIJ"
   },
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZVU66Gurr-"
   },
   "source": [
    "The labels indicating which sentence is positive and negative now go into the `labels` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = df['Product'] == 'Credit reporting, credit repair services, or other personal consumer reports'\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON6J_hzv2qbp"
   },
   "outputs": [],
   "source": [
    "labels.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "### Model Trainingn and Validation\n",
    "We now train and validate a LogisticRegression model. We will use \"coss_val_scores\" to perform a 5-fold cross validation and we choose \"accuracy\" as the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gG-EVWx4CzBc"
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "scores = cross_val_score(lr_clf, features, labels, scoring='accuracy', cv=5)\n",
    "print(\"Score is {:.2f} +- {:.2f}\".format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75oyhr3VxHoE"
   },
   "source": [
    "How good is this score? What can we compare it against? Let's first look at a dummy classifier. A dummy classifieris a classifier that makes predictions using simple rules. By dedfault, always predicts the class that maximizes the class prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnwgmqNG7i5l"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, features, labels)\n",
    "print(\"Score is {:.2f} +- {:.2f}\".format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lg4LOpoxSOR"
   },
   "source": [
    "So our model clearly does better than a dummy classifier. But, the data is not balanced and accuracy is not a good score. We should use confusion matrix to analyze the results and use the combination of precision and recall scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYeCQxV1JzxW"
   },
   "outputs": [],
   "source": [
    "# Calculate and print confusion matrix\n",
    "pred = cross_val_predict(lr_clf, features, labels, cv=5)\n",
    "conf_mx = confusion_matrix(labels, pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJQuqV6cnWQu"
   },
   "outputs": [],
   "source": [
    "# Calculate recall scores\n",
    "scores = cross_val_score(lr_clf, features, labels, scoring='recall', cv=5)\n",
    "print(\"Score is {:.2f} +- {:.2f}\".format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZapntsnBTS9"
   },
   "outputs": [],
   "source": [
    "# Calculate precision scores\n",
    "scores = cross_val_score(lr_clf, features, labels, scoring='precision', cv=5)\n",
    "print(\"Score is {:.2f} +- {:.2f}\".format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI9RGn3jR5tp"
   },
   "source": [
    "## Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj9CmEgtB0F5"
   },
   "outputs": [],
   "source": [
    "# convert classes to class numbers\n",
    "factorized = df['Product'].factorize()\n",
    "labels_multi = factorized[0]\n",
    "labels_text = factorized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WU2ylNOC5lF"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy scores\n",
    "scores = cross_val_score(lr_clf, features, labels_multi, scoring='accuracy', cv=5)\n",
    "print(\"Score is {:.2f} +- {:.2f}\".format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fStLliwDDApC"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy scores (Dummy Classifier)\n",
    "scores = cross_val_score(clf, features, labels_multi)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0y38_3LDGLD"
   },
   "outputs": [],
   "source": [
    "# Claculate and print confusion matrix\n",
    "pred = cross_val_predict(lr_clf, features, labels_multi, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DThn9YVEDqRI"
   },
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(labels_multi, pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2p9FVHmEIMb"
   },
   "outputs": [],
   "source": [
    "# Plot the multi-clkass confusion matrix.\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.xticks(range(9), labels_text, rotation=90)\n",
    "plt.yticks(range(9), labels_text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA5HGh0rSoKB"
   },
   "source": [
    "Try to interpret the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zEYWo3eTpKH"
   },
   "source": [
    "# Contact Information\n",
    "\n",
    "Congratulations, you have completed the tutorial for Day 2 of the Vector Institute 'AI Fundamentals and Ethical Considerations' program! Thank you for your time and attention.\n",
    "\n",
    "\n",
    "*   Instructor: Sayyed Nezhadi \n",
    "*   Program Director: Shingai Manjengwa \n",
    "*   Contact: learn@vectorinstitute.ai\n",
    "\n",
    "Never stop learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UklyutjeTpuC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Vector Institute - Text Classification Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
